/*
 * @author Akshay Dubey
 */
 
package assignment1_q2;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
public class Assignment1_q2 {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, " word count ");
        job.setJarByClass(Assignment1_q2.class);
        job.setMapperClass(countMapper.class);
        job.setReducerClass(countReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        FileInputFormat.addInputPath(job, new Path(args [0]));
        FileOutputFormat.setOutputPath(job, new Path(args [1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }        
    
}

/*
 * @author Akshay Dubey
 */
 
package assignment1_q2;

import java.io.IOException;
import java.util.HashSet;
import java.util.Set;
import java.util.StringTokenizer;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.Mapper;

public class countMapper extends Mapper <LongWritable, Text, Text, Text> {

    private Text word = new Text("Word count");
    private Text file = new Text();

    private Set<String> patternsToSkip = new HashSet<String>();

    public void map(LongWritable key, Text value, Context context
                    ) throws IOException, InterruptedException {
      file.set(((FileSplit) context.getInputSplit()).getPath().getName().toString());
      String line = value.toString().toLowerCase();
	  patternsToSkip.add(",");
	  patternsToSkip.add(":");
	  patternsToSkip.add(";");
	  patternsToSkip.add(".");
	  patternsToSkip.add("!");
	  patternsToSkip.add("*");
	  patternsToSkip.add("'");
	  patternsToSkip.add(">");
	  patternsToSkip.add("<");
	  patternsToSkip.add("-");
	  patternsToSkip.add("(");
	  patternsToSkip.add(")");
	  patternsToSkip.add("[");
	  patternsToSkip.add("]");
	  patternsToSkip.add("?");
	  patternsToSkip.add("#");
	  patternsToSkip.add("@");
	  patternsToSkip.add("/");
	  patternsToSkip.add("{");
	  patternsToSkip.add("}");
	  String patterns[] = new String[patternsToSkip.size()];
	  patterns = patternsToSkip.toArray(patterns);
	  
      for (String pattern : patterns) {
        line = line.replace(pattern, "");
      }
      
      StringTokenizer itr = new StringTokenizer(line);
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, file);
      }
    }   
}

/*
 * @author Akshay Dubey
 */
 
package assignment1_q2;

import java.io.IOException;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class countReducer extends Reducer <Text, Text, Text, Text> {

    enum CountersEnum { WORDS }
    @Override
    public void reduce(Text key, Iterable<Text> values, 
            Context context) throws IOException, InterruptedException {
        
        int sum = 0;
        boolean check=false;
        String fileName="";
        String textString="{";
        /*reducing by counting the filename against each key*/
        for (Text val : values){
            if(!check){
                fileName=val.toString();
                check=true;
            }
            if (fileName.equals(val.toString())){
                sum=sum+1; //for counting the number of occurance in each file
            }
            else{
                textString+=fileName + "="+sum +",";
                fileName=val.toString();
                sum=1;
            }

        }
        textString+= fileName + "="+sum +"}"; //making output pattern
        if (!textString.contains(",")){
			context.getCounter(CountersEnum.WORDS).increment(1);
            context.write(key, new Text(textString));
        }       
    }
}
